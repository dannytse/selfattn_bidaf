\documentclass{article}

\usepackage[final]{neurips_2019}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage{amsmath}

\newcommand{\note}[1]{\textcolor{blue}{{#1}}}

\title{
  Gated Self-Attention for SQuAD Question Answering \\
  \vspace{1em}
  \small{\normalfont Stanford CS224N Default Project}  % Select one and delete the other
}

\author{
  Danny Tse \\
  Department of Computer Science \\
  Stanford University \\
  \texttt{dannytse@stanford.edu} \\
   Mentor: Yuyan Wang \\
  % Examples of more authors
%   \And
%   Name \\
%   Department of Computer Science \\
%   Stanford University \\
%   \texttt{name@stanford.edu} \\
%   \And
%   Name \\
%   Department of Computer Science \\
%   Stanford University \\
%   \texttt{name@stanford.edu}
}

\begin{document}

\maketitle

\begin{abstract}
Machine comprehension and question answering are central questions in natural language processing, as they require modeling interactions between the passage and the question. In this paper, we build on the multi-stage hierarchical process (BiDAF) described in Seo et al. (2017)'s \textit{Bi-Directional Attention Flow for Machine Comprehension}. We utilize tools from the R-Net model described in \textit{R-Net: Machine Reading Comprehension with Self-Matching Networks}, testing different combinations of model components. We experiment with different types of encoding, such as using a Gated Recurrent Unit (GRU) or a Convolutional Neural Network (CNN), and attention mechanisms, such as comparing context-query attention layers and contemplating the usage of gates. We ultimately introduce a modified form of BiDAF which utilizes both a GRU and a CNN in its encoding layer, as well as BiDAF's context-query attention layer followed by R-Net's self-attention layer. We conduct various experiments on the SQuAD datasets, yielding competitive results on the CS224N SQuAD Leaderboard.
\end{abstract}



\section{Introduction}
A common misconception regarding language is that it has to do with people and words, not the concepts they represent. In fact, one can realize that language is in fact merely a social construct; the sequential representation of glyphs mean nothing to those who don't know the language! Thus, an interesting problem that arises is: how do we get machines to understand our language? More specifically, given a short passage of text, can a machine "read" this text and answer questions on it? This research question is particularly difficult as (1) there are many nuances to language in which a computer has to learn and (2) such training data is scarce, as though texts are abundant, it is difficult to come across texts along with question-answer pairs. 

In 2017, Seo et al. published \textit{Bi-Directional Attention Flow for Machine Comprehension} \cite{seo}, introducing a multi-stage hierarchical network (BiDAF) that represents the context at different levels, utilizing bidirectional attention to obtain a question-aware passage representation without early summarization. At time of publishing, this model achieved state-of-the-art results. Also in 2017, Microsoft Asia published \textit{R-Net: Machine Reading Comprehension with Self-Matching Networks} \cite{rnet}, introducing an end-to-end neural network model utilizing gated attention-based recurrent networks, self-matching attention, and a pointer-network output layer. The R-Net paper primarily focuses on two datasets: the Stanford Question Answering Dataset (SQuAD) and the Microsoft Machine Reading Comprehension (MS-MARCO) datasets. At time of publishing, the model outperformed strong baselines, beating the BiDAF model and taking over first place on the Stanford Question and Answering Dataset (SQuAD) leaderboard and claiming the best published results on the MS-MARCO dataset. 

In this paper, we wish to develop a similar neural network with reading comprehension skills - in other words, given a passage or document, the model could answer questions related to the text that require logical reasoning. To do so, we use methods and model architecture from the two papers discussed above. As both models (BiDAF and R-Net) have similar architecture in that it consists of some embedding layer and some context-question embedding layer, we combine the techniques discussed in the R-Net paper to improve upon the BiDAF model. Precisely, we use Seo et al.'s BiDAF model as our baseline, as well as expanding on it, applying a gated self-attention layer on top of the context-query matching in BiDAF. Ultimately, our model (Figure 1) consists of six layers: 1) an embedding layer (obtaining word and character embeddings) 2) an encoding layer for the embeddings, with a recurrent network and a convolutional neural network (for character embeddings only), applied separately to the passage and query, 3) a contextual embedding layer, which uses context clues from surrounding words to refine the embedding of the words, 4) a gated self-matching attention mechanism, aggregating evidence from the passage to infer the answer, 5) a modeling layer applied on the output of the gated self-matching layer, which utilizes a Recurrent Neural Network to scan the context, and 6) the output layer, which provides an answer to the query.

\begin{center}
\includegraphics[scale=0.5]{RNetBidaf.png}
Figure 1: Model Overview. Adapted from \textit{Bi-Directional Attention Flow for Machine Comprehension} and \textit{R-Net: Machine Reading Comprehension with Self-Matching Networks}.
\end{center}

Initially, we had decided to re-implement the entire R-Net model, but the full implementation required too much training time and device memory. As a result, we had to simplify both attention layers and the output layer, resulting in much weaker model performance. (Will be discussed further in Section 4.) Unfortunately, with extensive changes to the model, the pointer network appeared to no longer be a compatible output layer for the model -- we observed the loss stagnating, with performance below the BiDAF baseline. Thus, we began experimenting with different model combinations using our baseline, observing that the model architecture was similar - the BiDAF model had an extra modeling layer, while the R-Net model had an extra self-attention layer. We made sure to utilize what made R-Net strong - the usage of a recurrent network in the embedding layer, along with the gated self-attention mechanism, and incorporated these elements into the given BiDAF model. 




%The introduction explains the problem, why it's difficult, interesting, or important, how and why current methods succeed/fail at the problem, and explains the key ideas of your approach and results. Though an introduction covers similar material as an abstract, the introduction gives more space for motivation, detail, references to existing work, and to capture the reader's interest.

\section{Related Work}
Since the release of the SQuAD dataset, much progress has been made and we have seen the rise of several interesting NLP models. For example, character-level embeddings with Convolutional Neural Networks (CNN) were introduced in Kim (2014) \cite{kim}, where we embed characters into vectors, which are then fed in as 1D inputs to the CNN, and then max-pooled over the entire width to obtain a fixed-size vector for each word. In fact, we use this method in our model. 

Some work that R-Net builds on and improves on include Bahdanau et al. (2014)'s \textit{Neural Machine Translation by Jointly Learning to Align and Translate}, Rocktaschel et al. (2015)'s \textit{Reasoning about Entailment with Neural Attention}, Wang and Jiang (2016)'s \textit{Learning Natural Language Inference with LSTM}, and Wang and Jiang (2016)'s \textit{Machine Comprehension Using Match-LSTM and Answer Pointer}. When compared to previous attention-based recurrent network models like Bahdanau et al. (2014), Rocktaschel et al. (2015), the R-Net model utilizes a self-matching mechanism, where the model aggregates information from the text to infer the answer. This self-matching mechanism is implemented by using gated attention-based recurrent networks on the text against itself (versus text against question from earlier). 



\section{Approach}
To begin, we formally define the problem: given a context or passage and a query or question from SQuAD, we want our model to return an answer within the span of the passage/context, or indicate that no such answer exists. 

The baseline we use is a Bi-Directional Attention Flow (BiDAF) model on the word embeddings. This model is described in the Default Final Project Handout (SQuAD Track), and consists of a Word Embedding Layer, an Encoder Layer, a Context-Query Attention Layer, a Modeling Layer, and an Output Layer. Our main adjustment to this model is a change in the encoding layer (adding a ), as well as the addition of a self-attention layer (see Figure 1). We suspected that, since the self-attention mechanism allows hidden states to consider previous hidden states, this model can record long-distance dependencies, and as a result have more complete answers to questions. Variants of self-attention have proven to be vastly successful in modern natural language processing models (ex. usage of multi-headed self-attention in a Transformer Encoder-Decoder block introduced in Vaswani et al. (2017)'s \textit{Attention Is All You Need} \cite{vaswani}), thus in theory, this model should outperform our Bidirectional Attention Flow baseline. 

We describe the model architecture below. First, we obtain the query (question) and context (passage)'s word embeddings and then process their character embeddings through a convolutional neural network, followed by an encoding layer containing a bi-directional LSTM. Then, we apply a contextual embedding layer (as described in BiDAF), followed by R-Net's self-matching attention. This provides us with a question-aware representation for the passage. Then, we refine the passage representation with self-matching attention and feed it into the output layer to return the boundary of the answer span. The model architecture is: 

\begin{enumerate}
\item \textbf{Word and Character Embedding} Our character embedding layer maps each word to a high-dimensional vector space. Let $C = \{w_t^C\}_{t = 1}^T$ and  $Q = \{w_t^Q\}_{t = 1}^J$ represent the context paragraph and query respectively. As described in Kim (2014), we obtain the character-level embeddings of the words, which are then passed into a Convolutional Neural Network (CNN) as 1D inputs. Then, we max-pool over the entire width to obtain a fixed-sized vector $c_t$ for each word. Furthermore, this layer obtains the word embeddings of $C$ and $Q$ $e_t$ respectively, and returns the concatenation of these word embeddings and the fixed-size character embedding vectors obtained from the max-pool, or $[e_t^C, c_t^C]$ and $[e_t^Q, c_t^Q]$, for context and query respectively.  

\item \textbf{Question and Passage Encoder} Passing in the concatenated word-character embeddings from the first level, we first pass in $[e_t^C, c_t^C]$ and $[e_t^Q, c_t^Q]$ into a projection layer, projecting each embedding to have dimensionality $H$. In other words,
\begin{align*}
h^C_i &= W_{\text{proj}}[e_t^C, c_t^C] \in \mathbb{R}^H \\
h^Q_i &= W_{\text{proj}}[e_t^Q, c_t^Q] \in \mathbb{R}^H 
\end{align*}
Then, we apply a Highway Network to both $h^C_i$ and $h^Q_i$, as described in Srivastava (2015) \cite{srivi}, which given an input vector $h_i$, it computes
\begin{align*}
g &= \sigma(W_gh_i + b_g) \in \mathbb{R}^H \\
t &= \text{ReLU}(W_th_i + b_i) \in \mathbb{R}^H\\
h'_i &= g \odot t + (1 - g) \odot h_i \in \mathbb{R}^H	,
\end{align*}
 where $W_g, W_t, b_g, b_t$ are all learnable parameters. We apply the above transformation twice to each of $h^C_i$ and $h^Q_i$, each time with different learnable parameters. Finally, we apply a Gated Recurrent Unit (GRU) to encode the representations for words in the question and the passage. That is,
\begin{center}
$u_t^C = \text{GRU}_P(u_{i - 1}^P, h'^C_i))$

$u_t^Q = \text{GRU}_Q(u_{i - 1}^Q, h'^Q_i)$
\end{center}

Note that our baseline embedding and encoding layer is the exact same as described above, except the baseline only considers word-embeddings, and does not use a GRU layer.

\item \textbf{BiDAF Attention Layer} The BiDAF Attention Layer lies in the heart of the model - the main idea is that attention should flow both ways, from context to question and vice-versa. 

\item \textbf{Self-Matching Attention} Now, we are given the question-aware passage representation from previous layers, or $\{v_t^P\}_{t = 1}^n$. An issue with such a representation is that it lacks extensive knowledge of context. To circumvent this issue, the paper proposes using attention to match $\{v_t^P\}_{t = 1}^n$ against itself, obtaining $h_t^P$ which encodes evidence from words in the passage and their matching question information:
\begin{center}
$h_t^P = \text{GRU}(h_{t - 1}^P, [v_t^P, c_t])$
\end{center}
where $c_t$ is given by the attention-pooling vector of the whole passage $v^P$:
\begin{center}
$s_j^t = v^T\text{tanh}(W_v^Pu_j^P + W_u^{\bar{P}}u_t^P)$

$a_i^t = \text{exp}(s_i^t)/\sum_{j = 1}^n\text{exp}(s_j^t)$

$c_t = \sum_{i = 1}^n a_i^tv_i^P$
\end{center}
Once again, an additional gate as stated above is applied to $[v_t^P, c_t]$. 

\item \textbf{Output Layer} Finally, pointer networks are used to predict the start and end position of the answer. Attention-pooling over the question representation is used to generate the initial hidden vector for the pointer network. Given $\{h_t^P\}_{t = 1}^n$ from the previous layers, we utilize the attention mechanism to select start and end positions $p^1$ and $p^2$ from the passage:
\begin{center}
$s_j^t = v^T\text{tanh}(W_h^Ph_j^P + W_h^{a}h_{t - 1}^a)$

$a_i^t = \text{exp}(s_i^t)/\sum_{j = 1}^n\text{exp}(s_j^t)$

$p_t = \text{argmax}(a_1^t, \dots, a_n^t)$
\end{center}
where
\begin{center}
$c_t = \sum_{i = 1}^n a_i^th_i^P$

$h_t^a = \text{GRU}(h_{t - 1}^a, c_t)$
\end{center}
Then, using $h^{t - 1}_a$ as the initial hidden state and $r^Q$ as the initial state,
\begin{center}
$s_j = v^T\text{tanh}(W_u^Qu_j^Q + W_v^QV_r^Q)$

$a_i = \text{exp}(s_i)/\sum_{j = 1}^n\text{exp}(s_j)$

$r^Q = \sum_{i = 1}^m a_iu_i^Q$
\end{center}



\end{enumerate}

Though the paper describes exactly what layers to use, and what model architecture components to incorporate, we may experiment with other adjustments that have proven to work. One example of such an adjustment is described in Seo et al. (2017)'s \textit{Bi-Directional Attention Flow for Machine Comprehension} \cite{seo}, where a Convolutional Neural Network (CNN) Layer is utilized to process character embeddings. In this modification, we simply obtain the character-level embedding of each word using a CNN, instead of a GRU. Characters are embedded into vectors whose size is the input-channel size of the CNN, which are then fed as 1-dimensional inputs. Then, we apply a max-pool on the CNN's outputs over the entire width to obtain a fixed-size vector for each word.



\section{Experiments}
This section contains the following.

\subsection{Data}

As data, we use the Stanford Question Answering Dataset 2.0 (SQuAD 2.0), which has the splits train (129,941 examples), dev (6078 examples), and split (5915 examples). In this dataset, passages are selected from the English Wikipedia (usually 100-150 words), questions are crowd-sourced, and each answer is either contained within the passage, or does not exist at all (not answerable). Also, each answerable SQuAD question has three answers, each from a different crowd worker. 

The task is, given a passage \textbf{P} and a question \textbf{Q} from SQuAD, we want our model to return an answer \textbf{A} within the span of the passage, or indicate that no such answer exists. For example, as listed in the paper, an example question-answer pair from SQuAD is as follows:

\textit{Passage:}  Tesla later approached Morgan to ask for more funds to build a more powerful transmitter. \textbf{When asked where all the money had gone, Tesla responded by saying that he was affected by the Panic of 1901, which he (Morgan) had caused}. Morgan was shocked by the reminder of his part in the stock market crash and by Teslaâ€™s breach of contract by asking for more funds. Tesla wrote another plea to Morgan, but it was also fruitless. Morgan still owed Tesla money on the original agreement, and Tesla had been facing foreclosure even before construction of the tower began.

\textit{Question:} On what did Tesla blame for the loss of the initial money?

\textit{Answer:} Panic of 1901.

\subsection{Evaluation method}

To evaluate model performance, we use two well-defined, numerical, automatic evaluation metrics: Exact Match (EM) and F1 score. The Exact Match (EM) captures the percentage of the prediction that completely matches with one of the ground truth answers. For example, if the ground truth was "Panic of 1901" and the model responded with "1901", then the ground truth score would be 0. On the other hand, the F1 score is less strict - it is the harmonic mean of precision and recall. An example of an F1 score is with "1901" (model answer) and "Panic of 1901" (ground truth). Since the model answer is completely contained in the ground truth, it would have 100\% precision, though only 33.3\% recall, as it only included one out of the three words in the ground truth answer. Thus we have an F1 score of $2 \times 100 \times 33.3/(100 + 33.3)  \approx 50.0\%$. We compare these evaluations against the BiDAF baseline's performance, hoping for a moderate increase given the effectiveness of the R-Net model. 


\subsection{Experimental details}
Report how you ran your experiments (e.g. model configurations, learning rate, training time, etc.)

\subsection{Results}
Report the quantitative results that you have found so far. Use a table or plot to compare results and compare against baselines.

In experimentation thus far, we have run three versions of the model: (1) the baseline, (2) baseline, but replacing the first word embedding layer with a word + character embedding layer, which is processed through a CNN as described above, and (3) baseline, but replacing the first word embedding layer with a word + character embedding layer, passed through the Gated Recurrent Unit described in the model description above. We used default baseline hyper-parameters for all three test runs, and they ran at different times, with iteration (3) taking almost four times as long as the baseline. Furthermore, we observed that model (3) performed slightly worse than the baseline, and model (2) performed slightly better than the baseline. We observed the following results:
\begin{center}
\begin{tabular}{ |p{3cm}||p{3cm}|p{3cm}|  }
 \hline
 \multicolumn{3}{|c|}{F1 and EM Scores} \\
 \hline
 \textbf{Model} & \textbf{EM Score} & \textbf{F1 Score}\\
 \hline
Baseline & 57.889 & 60.965 \\
Baseline + CNN char lv. emb. & \textbf{57.973} & \textbf{61.381} \\
Baseline + GRU char lv. emb. & 52.731 & 55.390\\
RNet + BiDAF Output & 54.327 & 57.145\\
 \hline
\end{tabular}
\end{center}


\begin{itemize}
    \item If you're a default project team, you should \textbf{report the F1 and EM scores you obtained on the test leaderboard} in this section. Make it clear whether you are on the non-PCE or PCE leaderboard. You can also report dev set results if you like. 
    \item Comment on your quantitative results. Are they what you expected? Better than you expected? Worse than you expected? Why do you think that is? What does that tell you about your approach?
\end{itemize}

\section{Analysis}
Your report should include \textit{qualitative evaluation}. That is, try to understand your system (e.g. how it works, when it succeeds and when it fails) by inspecting key characteristics or outputs of your model.

\section{Conclusion}
Summarize the main findings of your project, and what you have learnt. Highlight your achievements, and note the primary limitations of your work. If you like, you can describe avenues for future work.


\bibliographystyle{unsrt}
\bibliography{references}
%
%\appendix
%
%\section{Appendix (optional)}
%If you wish, you can include an appendix, which should be part of the main PDF, and does not count towards the 6-8 page limit.
%Appendices can be useful to supply extra details, examples, figures, results, visualizations, etc., that you couldn't fit into the main paper. However, your grader \textit{does not} have to read your appendix, and you should assume that you will be graded based on the content of the main part of your paper only.

\end{document}
